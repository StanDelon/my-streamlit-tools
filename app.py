import streamlit as st
import re
from collections import Counter, defaultdict
import pymorphy3
import pandas as pd
import hashlib
import base64
from io import StringIO
import chardet

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
    page_icon="üîß",
    layout="wide"
)

# ===== –û–±—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====
def parse_semantic_core(text: str) -> List[str]:
    """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ –≤ —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑"""
    return [line.strip() for line in text.split('\n') if line.strip()]

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 1: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ =====
def prepare_morph_analyzer():
    return pymorphy3.MorphAnalyzer()

def normalize_word(word, morph, force_exact=False):
    if force_exact or word.startswith('!'):
        return word.lstrip('!').lower()
    parsed = morph.parse(word.lower())[0]
    return parsed.normal_form

def parse_exclude_input(user_input):
    if not user_input:
        return []
    
    if '\n' in user_input:
        lines = [line.strip() for line in user_input.split('\n') if line.strip()]
        return [item for line in lines for item in parse_exclude_input(line)]
    
    if '(' in user_input and ')' in user_input and '|' in user_input:
        parts = []
        remaining = user_input
        while '(' in remaining and ')' in remaining:
            before = remaining[:remaining.index('(')]
            variant_part = remaining[remaining.index('(')+1:remaining.index(')')]
            after = remaining[remaining.index(')')+1:]
            
            if before.strip():
                parts.extend(parse_exclude_input(before))
            parts.extend(v.strip() for v in variant_part.split('|') if v.strip())
            remaining = after
        if remaining.strip():
            parts.extend(parse_exclude_input(remaining))
        return parts
    
    delimiters = [',', '|'] if '|' in user_input else [',']
    for delim in delimiters:
        if delim in user_input:
            return [item.strip() for item in user_input.split(delim) if item.strip()]
    
    return [user_input.strip()] if user_input.strip() else []

def get_exclusion_patterns(exclude_list, morph):
    patterns = []
    for item in exclude_list:
        item = item.strip()
        if not item:
            continue
            
        if item.startswith('/') and item.endswith('/'):
            pattern = item[1:-1]
            patterns.append((pattern, True, True))
            continue
            
        force_exact = item.startswith('!')
        if force_exact:
            item = item[1:]
            
        if '*' in item:
            escaped = re.escape(item)
            pattern = escaped.replace(r'\*', r'\w*')
            pattern = r'\b' + pattern + r'\b'
            patterns.append((pattern, False, True))
        else:
            words = re.findall(r'\w+', item.lower())
            normalized_words = [normalize_word(w, morph, force_exact) for w in words]
            pattern = r'\b' + r'\s+'.join(normalized_words) + r'\b'
            patterns.append((pattern, False, False))
    
    return patterns

def should_exclude(word, exclude_patterns):
    word_lower = word.lower()
    for pattern, is_regex, is_wildcard in exclude_patterns:
        try:
            if is_regex:
                if re.search(pattern, word_lower, re.IGNORECASE):
                    return True
            elif is_wildcard:
                if re.search(pattern, word_lower, re.IGNORECASE):
                    return True
            else:
                if re.fullmatch(pattern, word_lower, re.IGNORECASE):
                    return True
        except re.error:
            continue
    return False

def process_phrases(phrases, exclude_patterns, morph, min_word_length=3):
    words_counter = Counter()
    for phrase in phrases:
        words = re.findall(r'\b\w+\b', phrase.lower())
        for word in words:
            if len(word) >= min_word_length and not should_exclude(word, exclude_patterns):
                normalized = normalize_word(word, morph)
                words_counter[normalized] += 1
    
    return [word for word, count in sorted(words_counter.items(), key=lambda x: (-x[1], x[0]))]

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 2: –î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ =====
def build_hierarchy(phrases: List[str]) -> Dict[str, Dict]:
    """–°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ —Ñ—Ä–∞–∑"""
    hierarchy = defaultdict(lambda: {
        'count': 0,
        'phrases': [],
        'subgroups': defaultdict(lambda: {'count': 0, 'phrases': []})
    })
    
    for phrase in phrases:
        words = phrase.split()
        
        possible_groups = []
        for i in range(1, len(words)+1):
            group = ' '.join(words[:i])
            possible_groups.append((group, i))
        
        possible_groups.sort(key=lambda x: -x[1])
        
        added = False
        for group, _ in possible_groups:
            if group in hierarchy:
                remaining = ' '.join(words[len(group.split()):]).strip()
                if remaining:
                    hierarchy[group]['subgroups'][remaining]['phrases'].append(phrase)
                    hierarchy[group]['subgroups'][remaining]['count'] += 1
                else:
                    hierarchy[group]['phrases'].append(phrase)
                    hierarchy[group]['count'] += 1
                added = True
                break
        
        if not added:
            hierarchy[phrase]['phrases'].append(phrase)
            hierarchy[phrase]['count'] += 1
    
    def convert_to_regular(d):
        if isinstance(d, defaultdict):
            d = {k: convert_to_regular(v) for k, v in d.items()}
        return d
    
    hierarchy = convert_to_regular(hierarchy)
    
    sorted_hierarchy = {}
    for group in sorted(hierarchy.keys(), key=lambda x: -hierarchy[x]['count']):
        sorted_subgroups = {}
        for subgroup in sorted(hierarchy[group]['subgroups'].keys(), 
                             key=lambda x: -hierarchy[group]['subgroups'][x]['count']):
            sorted_subgroups[subgroup] = hierarchy[group]['subgroups'][subgroup]
        
        sorted_hierarchy[group] = {
            'count': hierarchy[group]['count'],
            'phrases': hierarchy[group]['phrases'],
            'subgroups': sorted_subgroups
        }
    
    return sorted_hierarchy

def render_tree_node(group: str, data: Dict, excluded_phrases: Set[str], level: int = 0):
    """–†–µ–Ω–¥–µ—Ä–∏—Ç —É–∑–µ–ª –¥–µ—Ä–µ–≤–∞ —Å —á–µ–∫–±–æ–∫—Å–æ–º"""
    indent = "    " * level
    container = st.container()
    
    with container:
        cols = st.columns([1, 4])
        with cols[0]:
            excluded = st.checkbox(
                f"{group} ({data['count']})",
                value=all(p in excluded_phrases for p in data['phrases']),
                key=f"node_{level}_{hash(group)}"
            )
        
        if excluded:
            excluded_phrases.update(data['phrases'])
        else:
            excluded_phrases.difference_update(data['phrases'])
        
        if data['subgroups']:
            with st.expander(f"{indent}–ü–æ–¥–≥—Ä—É–ø–ø—ã ({len(data['subgroups'])})"):
                for subgroup, sub_data in data['subgroups'].items():
                    render_tree_node(subgroup, sub_data, excluded_phrases, level+1)
        
        if data['phrases']:
            with st.expander(f"{indent}–§—Ä–∞–∑—ã ({len(data['phrases'])})"):
                for phrase in data['phrases']:
                    st.write(f"{indent}{phrase}")

def semantic_core_grouper():
    """–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
    st.header("üå≥ –î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞")
    
    example_phrases = """
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio 120 80 215
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio ks 6690m
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã –≤–∏–ª–∞–¥–∂–∏–æ
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã –≤–∏–ª–ª–∞–¥–∂–∏–æ
—Ä–µ–º–æ–Ω—Ç –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã
—Ä–µ–º–æ–Ω—Ç –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio
—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã
—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã –≤–∏–ª–∞–¥–∂–∏–æ
"""
    
    st.subheader("1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ")
    input_type = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö", ["–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö", "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"], key="data_source")
    
    if input_type == "–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö":
        phrases = parse_semantic_core(example_phrases)
    else:
        uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª (.txt)", type=['txt'], key="file_uploader")
        if uploaded_file:
            text = uploaded_file.read().decode('utf-8')
            phrases = parse_semantic_core(text)
        else:
            st.warning("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è")
            return
    
    if not phrases:
        st.error("–ù–µ—Ç —Ñ—Ä–∞–∑ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
        return
    
    st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(phrases)} —Ñ—Ä–∞–∑")
    
    st.subheader("2. –î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
    hierarchy = build_hierarchy(phrases)
    
    if 'excluded_phrases' not in st.session_state:
        st.session_state.excluded_phrases = set()
    
    tree_container = st.container()
    with tree_container:
        for group, data in hierarchy.items():
            render_tree_node(group, data, st.session_state.excluded_phrases)
    
    st.subheader("3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è", key="clear_all"):
            st.session_state.excluded_phrases = set()
            st.rerun()
    with col2:
        if st.button("–ò—Å–∫–ª—é—á–∏—Ç—å –≤—Å–µ —Ñ—Ä–∞–∑—ã", key="exclude_all"):
            st.session_state.excluded_phrases = set(phrases)
            st.rerun()
    
    st.subheader("4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    tab1, tab2 = st.tabs(["–ò—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã", "–û—Å—Ç–∞–≤—à–∏–µ—Å—è —Ñ—Ä–∞–∑—ã"])
    
    with tab1:
        if st.session_state.excluded_phrases:
            st.write("\n".join(sorted(st.session_state.excluded_phrases)))
        else:
            st.info("–ù–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑")
    
    with tab2:
        remaining = set(phrases) - st.session_state.excluded_phrases
        if remaining:
            st.write("\n".join(sorted(remaining)))
        else:
            st.warning("–í—Å–µ —Ñ—Ä–∞–∑—ã –∏—Å–∫–ª—é—á–µ–Ω—ã")

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 3: –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ =====
def hash_phone(phone):
    phone_str = str(phone).strip()
    if not phone_str:
        return ""
    digits = re.sub(r'\D', '', phone_str)
    if not digits:
        return ""
    return hashlib.sha256(digits.encode('utf-8')).hexdigest()

def get_table_download_link(df):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è DataFrame"""
    csv = df.to_csv(index=False, encoding='utf-8-sig')
    b64 = base64.b64encode(csv.encode('utf-8-sig')).decode()
    return f'<a href="data:file/csv;base64,{b64}" download="processed_data.csv">–°–∫–∞—á–∞—Ç—å CSV</a>'

# ===== –ì–ª–∞–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å =====
def main():
    st.title("üîß –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã")
    st.markdown("---")

    tool_options = [
        "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤",
        "–î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞", 
        "–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤"
    ]

    tool = st.sidebar.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
        tool_options,
        index=0
    )

    if tool == "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤":
        st.header("üìâ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ –¥–ª—è —Ä–µ–∫–ª–∞–º—ã")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("1. –í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            phrases = st.text_area(
                "–ö–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏",
                height=200,
                help="–í—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
                key="phrases_input"
            )
            
        with col2:
            st.subheader("2. –£–∫–∞–∂–∏—Ç–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
            exclude = st.text_area(
                "–§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2, (–≤–∞—Ä–∏–∞–Ω—Ç1|–≤–∞—Ä–∏–∞–Ω—Ç2), /—Ä–µ–≥—ç–∫—Å–ø/",
                height=200,
                help="""–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
                - –ó–∞–ø—è—Ç—ã–µ: —Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2
                - –í–∞—Ä–∏–∞–Ω—Ç—ã: (–≤–∏–ª–ª–∞–¥–∂–∏–æ|villagio)
                - –ú–∞—Å–∫–∏: —Ä–µ–º–æ–Ω—Ç*
                - –†–µ–≥—É–ª—è—Ä–∫–∏: /—Ü–µ–Ω–∞|—Å—Ç–æ–∏–º–æ—Å—Ç—å/
                - –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: !–æ–Ω–ª–∞–π–Ω""",
                key="exclude_input"
            )
        
        st.markdown("---")
        st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        col_set1, col_set2 = st.columns(2)
        with col_set1:
            min_length = st.slider(
                "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞",
                min_value=1,
                max_value=10,
                value=3,
                help="–°–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –±—É–¥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è",
                key="min_length_slider"
            )
        
        with col_set2:
            show_stats = st.checkbox(
                "–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º",
                value=True,
                help="–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤",
                key="show_stats_checkbox"
            )
        
        if st.button("üöÄ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞", type="primary", key="generate_minus_words"):
            if not phrases:
                st.error("–í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
            else:
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
                    try:
                        morph = prepare_morph_analyzer()
                        exclude_list = parse_exclude_input(exclude)
                        exclude_patterns = get_exclusion_patterns(exclude_list, morph)
                        phrases_list = [p.strip() for p in phrases.split('\n') if p.strip()]
                        
                        minus_words = process_phrases(phrases_list, exclude_patterns, morph, min_length)
                        result = " ".join([f"-{word}" for word in minus_words])
                        
                        st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
                        
                        tab1, tab2 = st.tabs(["–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç", "–°–ø–∏—Å–æ–∫ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è"])
                        
                        with tab1:
                            st.text_area(
                                "–†–µ–∑—É–ª—å—Ç–∞—Ç (—Ç–µ–∫—Å—Ç)",
                                value=result,
                                height=100,
                                key="result_text"
                            )
                        
                        with tab2:
                            st.text_area(
                                "–†–µ–∑—É–ª—å—Ç–∞—Ç (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)",
                                value="\n".join([f"-{word}" for word in minus_words]),
                                height=200,
                                key="result_list"
                            )
                        
                        if show_stats:
                            st.markdown("---")
                            st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º")
                            
                            words_count = Counter()
                            for phrase in phrases_list:
                                words = re.findall(r'\b\w+\b', phrase.lower())
                                for word in words:
                                    if len(word) >= min_length:
                                        normalized = normalize_word(word, morph)
                                        if not should_exclude(normalized, exclude_patterns):
                                            words_count[normalized] += 1
                            
                            df_stats = pd.DataFrame.from_dict(words_count, orient='index', columns=['–ß–∞—Å—Ç–æ—Ç–∞'])
                            df_stats = df_stats.sort_values(by='–ß–∞—Å—Ç–æ—Ç–∞', ascending=False)
                            
                            st.dataframe(df_stats.head(50))
                            st.bar_chart(df_stats.head(20))
                    
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

    elif tool == "–î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞":
        semantic_core_grouper()

    elif tool == "–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤":
        st.header("üìû –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤")
        
        uploaded_file = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (Excel –∏–ª–∏ CSV)",
            type=["xlsx", "xls", "csv"],
            accept_multiple_files=False,
            key="phone_hash_uploader"
        )
        
        if uploaded_file:
            try:
                if uploaded_file.name.endswith('.csv'):
                    content = uploaded_file.getvalue()
                    result = chardet.detect(content)
                    encoding = result['encoding'] if result['confidence'] > 0.7 else 'utf-8'
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding=encoding)
                else:
                    df = pd.read_excel(uploaded_file)
                
                st.success(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
                    st.dataframe(df.head())
                
                with col2:
                    st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                    phone_column = st.selectbox(
                        "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏",
                        df.columns,
                        index=0,
                        key="phone_column_select"
                    )
                    
                    new_column_name = st.text_input(
                        "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ö—ç—à–∞–º–∏",
                        value="phone_hash",
                        key="new_column_name_input"
                    )
                    
                    if st.button("üîí –•—ç—à–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ", type="primary", key="hash_phones_button"):
                        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                            try:
                                result_df = df.copy()
                                result_df[new_column_name] = result_df[phone_column].apply(hash_phone)
                                st.success("–ì–æ—Ç–æ–≤–æ! –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
                                st.dataframe(result_df.head())
                                st.markdown(get_table_download_link(result_df), unsafe_allow_html=True)
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")

    # –§—É—Ç–µ—Ä
    st.markdown("---")
    st.markdown("""
    <style>
    .footer {
        font-size: small;
        color: gray;
        text-align: center;
    }
    </style>
    <div class="footer">
        –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã | 2023
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
