import streamlit as st
import re
from collections import Counter
import pymorphy3
import pandas as pd
import hashlib
import base64
import io
import chardet
from io import BytesIO

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤",
    page_icon="üîç",
    layout="wide"
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
try:
    morph = pymorphy3.MorphAnalyzer()
except ImportError:
    st.error("–û—à–∏–±–∫–∞: –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –º–æ–¥—É–ª—å pymorphy3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –∫–æ–º–∞–Ω–¥–æ–π: pip install pymorphy3")
    st.stop()

# –°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è —Ç–æ–ø–æ–Ω–∏–º–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
TOPONYMS = {
    '–º–æ—Å–∫–≤': '–º–æ—Å–∫–≤–∞',
    '–ø–∏—Ç–µ—Ä': '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥',
    '—Å–ø–±': '—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥',
    '–∫–∞–∑–∞–Ω': '–∫–∞–∑–∞–Ω—å',
    '–Ω–æ–≤–æ—Å–∏–±': '–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫',
    '–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥': '–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥',
    '–Ω–∏–∂–Ω': '–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥',
    '—Ä–æ—Å—Ç–æ–≤': '—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É'
}

# ===== –§—É–Ω–∫—Ü–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ =====
def normalize_word(word, force_exact=False):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–æ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ —Å —É—á–µ—Ç–æ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    if force_exact or word.startswith('!'):
        return word.lstrip('!').lower()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ–ø–æ–Ω–∏–º—ã
    word_lower = word.lower()
    for short, full in TOPONYMS.items():
        if word_lower.startswith(short):
            return full
    
    # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    parsed = morph.parse(word_lower)[0]
    
    # –î–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂
    if 'NOUN' in parsed.tag:
        return parsed.inflect({'nomn'}).word if parsed.inflect({'nomn'}) else parsed.normal_form
    
    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
    return parsed.normal_form

def parse_exclude_input(user_input):
    """–ü–∞—Ä—Å–∏—Ç –≤–≤–æ–¥ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    if not user_input:
        return []
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞
    if '\n' in user_input:
        lines = [line.strip() for line in user_input.split('\n') if line.strip()]
        return [item for line in lines for item in parse_exclude_input(line)]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ | –∏–ª–∏ –∑–∞–ø—è—Ç—ã–µ
    delimiters = ['|', ','] if '|' in user_input else [',']
    for delim in delimiters:
        if delim in user_input:
            return [item.strip() for item in user_input.split(delim) if item.strip()]
    
    return [user_input.strip()] if user_input.strip() else []

def get_exclusion_patterns(exclude_list):
    """–°–æ–∑–¥–∞–µ—Ç regex-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    patterns = []
    for item in exclude_list:
        item = item.strip()
        if not item:
            continue
            
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        force_exact = item.startswith('!')
        if force_exact:
            item = item[1:]
            
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã
        if ' ' in item:
            # –î–ª—è —Ñ—Ä–∞–∑ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
            words = item.split()
            normalized_words = [normalize_word(w, force_exact) for w in words]
            pattern = r'\b' + r'\s+'.join(normalized_words) + r'\b'
        else:
            # –î–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
            normalized = normalize_word(item, force_exact)
            pattern = r'\b' + re.escape(normalized) + r'\b'
        
        patterns.append(pattern)
    return patterns

def should_exclude(word, exclude_patterns):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–ª—é—á–∞—Ç—å —Å–ª–æ–≤–æ"""
    for pattern in exclude_patterns:
        if re.search(pattern, word, re.IGNORECASE):
            return True
    return False

def process_phrases(phrases, exclude_patterns, min_word_length=3):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ—Ä–∞–∑—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞"""
    words_counter = Counter()
    
    for phrase in phrases:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ü–∏—Ñ—Ä—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø—ë–Å]{3,}\b', phrase.lower())
        for word in words:
            normalized = normalize_word(word)
            if not should_exclude(normalized, exclude_patterns):
                words_counter[normalized] += 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ –∞–ª—Ñ–∞–≤–∏—Ç—É
    return sorted(words_counter.keys(), key=lambda x: (-words_counter[x], x))

# ===== Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å =====
st.title("üîç –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π")
st.markdown("---")

# –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
col1, col2 = st.columns(2)

with col1:
    st.subheader("1. –í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    phrases_input = st.text_area(
        "–ö–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏",
        height=200,
        help="–ü—Ä–∏–º–µ—Ä:\n–∫—É–ø–∏—Ç—å —Ü–≤–µ—Ç—ã –≤ –º–æ—Å–∫–≤–µ\n–¥–æ—Å—Ç–∞–≤–∫–∞ —Ü–≤–µ—Ç–æ–≤ –≤ –ø–∏—Ç–µ—Ä"
    )

with col2:
    st.subheader("2. –£–∫–∞–∂–∏—Ç–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
    exclude_input = st.text_area(
        "–§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2, (–≤–∞—Ä–∏–∞–Ω—Ç1|–≤–∞—Ä–∏–∞–Ω—Ç2)",
        height=200,
        help="–ü—Ä–∏–º–µ—Ä:\n—Ü–≤–µ—Ç—ã, !—Ç–æ—á–Ω–æ–µ, –¥–æ—Å—Ç–∞–≤–∫*, (–º–æ—Å–∫–≤|–ø–∏—Ç–µ—Ä)"
    )

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
st.markdown("---")
st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

min_length = st.slider(
    "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞",
    min_value=2,
    max_value=10,
    value=3,
    help="–°–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –±—É–¥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è"
)

show_stats = st.checkbox(
    "–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º",
    value=True,
    help="–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤"
)

# –û–±—Ä–∞–±–æ—Ç–∫–∞
if st.button("üöÄ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞", type="primary"):
    if not phrases_input:
        st.error("–í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
    else:
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
            try:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                phrases_list = [p.strip() for p in phrases_input.split('\n') if p.strip()]
                exclude_list = parse_exclude_input(exclude_input)
                exclude_patterns = get_exclusion_patterns(exclude_list)
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∏–Ω—É—Å-—Å–ª–æ–≤
                minus_words = process_phrases(phrases_list, exclude_patterns, min_length)
                
                # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
                
                # –í–∫–ª–∞–¥–∫–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ –≤—ã–≤–æ–¥–∞
                tab1, tab2 = st.tabs(["–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç", "–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç"])
                
                with tab1:
                    st.text_area(
                        "–ú–∏–Ω—É—Å-—Å–ª–æ–≤–∞ (–¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è)",
                        value=" ".join([f"-{word}" for word in minus_words]),
                        height=100
                    )
                
                with tab2:
                    # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
                    words_freq = Counter()
                    for phrase in phrases_list:
                        words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø—ë–Å]{3,}\b', phrase.lower())
                        for word in words:
                            normalized = normalize_word(word)
                            if not should_exclude(normalized, exclude_patterns):
                                words_freq[normalized] += 1
                    
                    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ—Ç—á–µ—Ç–∞
                    df_report = pd.DataFrame.from_dict(words_freq, orient='index', columns=['–ß–∞—Å—Ç–æ—Ç–∞'])
                    df_report = df_report.sort_values(by='–ß–∞—Å—Ç–æ—Ç–∞', ascending=False)
                    
                    st.dataframe(df_report)
                    st.bar_chart(df_report.head(20))
                
                st.info(f"–í—Å–µ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(minus_words)} –º–∏–Ω—É—Å-—Å–ª–æ–≤")
            
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.caption("–í–µ—Ä—Å–∏—è 3.0 | –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π")
