import streamlit as st
import re
from collections import Counter, defaultdict
import pymorphy3
import pandas as pd
import hashlib
import base64
from io import StringIO
import chardet

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
    page_icon="üîß",
    layout="wide"
)

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 1: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ =====
def prepare_morph_analyzer():
    return pymorphy3.MorphAnalyzer()

def normalize_word(word, morph, force_exact=False):
    if force_exact or word.startswith('!'):
        return word.lstrip('!').lower()
    parsed = morph.parse(word.lower())[0]
    return parsed.normal_form

def parse_exclude_input(user_input):
    if not user_input:
        return []
    
    if '\n' in user_input:
        lines = [line.strip() for line in user_input.split('\n') if line.strip()]
        return [item for line in lines for item in parse_exclude_input(line)]
    
    if '(' in user_input and ')' in user_input and '|' in user_input:
        parts = []
        remaining = user_input
        while '(' in remaining and ')' in remaining:
            before = remaining[:remaining.index('(')]
            variant_part = remaining[remaining.index('(')+1:remaining.index(')')]
            after = remaining[remaining.index(')')+1:]
            
            if before.strip():
                parts.extend(parse_exclude_input(before))
            parts.extend(v.strip() for v in variant_part.split('|') if v.strip())
            remaining = after
        if remaining.strip():
            parts.extend(parse_exclude_input(remaining))
        return parts
    
    delimiters = [',', '|'] if '|' in user_input else [',']
    for delim in delimiters:
        if delim in user_input:
            return [item.strip() for item in user_input.split(delim) if item.strip()]
    
    return [user_input.strip()] if user_input.strip() else []

def get_exclusion_patterns(exclude_list, morph):
    patterns = []
    for item in exclude_list:
        item = item.strip()
        if not item:
            continue
            
        if item.startswith('/') and item.endswith('/'):
            pattern = item[1:-1]
            patterns.append((pattern, True, True))
            continue
            
        force_exact = item.startswith('!')
        if force_exact:
            item = item[1:]
            
        if '*' in item:
            escaped = re.escape(item)
            pattern = escaped.replace(r'\*', r'\w*')
            pattern = r'\b' + pattern + r'\b'
            patterns.append((pattern, False, True))
        else:
            words = re.findall(r'\w+', item.lower())
            normalized_words = [normalize_word(w, morph, force_exact) for w in words]
            pattern = r'\b' + r'\s+'.join(normalized_words) + r'\b'
            patterns.append((pattern, False, False))
    
    return patterns

def should_exclude(word, exclude_patterns):
    word_lower = word.lower()
    for pattern, is_regex, is_wildcard in exclude_patterns:
        try:
            if is_regex:
                if re.search(pattern, word_lower, re.IGNORECASE):
                    return True
            elif is_wildcard:
                if re.search(pattern, word_lower, re.IGNORECASE):
                    return True
            else:
                if re.fullmatch(pattern, word_lower, re.IGNORECASE):
                    return True
        except re.error:
            continue
    return False

def process_phrases(phrases, exclude_patterns, morph, min_word_length=3):
    words_counter = Counter()
    for phrase in phrases:
        words = re.findall(r'\b\w+\b', phrase.lower())
        for word in words:
            if len(word) >= min_word_length and not should_exclude(word, exclude_patterns):
                normalized = normalize_word(word, morph)
                words_counter[normalized] += 1
    
    return [word for word, count in sorted(words_counter.items(), key=lambda x: (-x[1], x[0]))]

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 2: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ =====
def parse_semantic_core(text):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑"""
    return [line.strip() for line in text.split('\n') if line.strip()]

def build_hierarchy(phrases):
    """–°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é —Ñ—Ä–∞–∑ —Å —É—á–µ—Ç–æ–º —á–∞—Å—Ç–æ—Ç—ã –≤—Ö–æ–∂–¥–µ–Ω–∏–π"""
    hierarchy = defaultdict(lambda: {
        'count': 0,
        'phrases': [],
        'subgroups': defaultdict(lambda: {'count': 0, 'phrases': []})
    })
    
    for phrase in phrases:
        words = phrase.split()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≥—Ä—É–ø–ø—ã –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º
        possible_groups = []
        for i in range(1, len(words)+1):
            group = ' '.join(words[:i])
            possible_groups.append((group, i))
        
        possible_groups.sort(key=lambda x: -x[1])
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—É –≤ —Å–∞–º—É—é –¥–ª–∏–Ω–Ω—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –≥—Ä—É–ø–ø—É
        added = False
        for group, _ in possible_groups:
            if group in hierarchy:
                remaining = ' '.join(words[len(group.split()):]).strip()
                if remaining:
                    hierarchy[group]['subgroups'][remaining]['phrases'].append(phrase)
                    hierarchy[group]['subgroups'][remaining]['count'] += 1
                else:
                    hierarchy[group]['phrases'].append(phrase)
                    hierarchy[group]['count'] += 1
                added = True
                break
        
        if not added:
            hierarchy[phrase]['phrases'].append(phrase)
            hierarchy[phrase]['count'] += 1
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º defaultdict –≤ –æ–±—ã—á–Ω—ã–µ dict –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    def convert_to_regular_dict(d):
        if isinstance(d, defaultdict):
            d = dict(d)
            for k, v in d.items():
                d[k] = convert_to_regular_dict(v)
        elif isinstance(d, dict):
            for k, v in d.items():
                d[k] = convert_to_regular_dict(v)
        return d
    
    hierarchy = convert_to_regular_dict(hierarchy)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–∂–¥–µ–Ω–∏–π
    sorted_hierarchy = {}
    for group in sorted(hierarchy.keys(), key=lambda x: -hierarchy[x]['count']):
        sorted_subgroups = {}
        for subgroup in sorted(hierarchy[group]['subgroups'].keys(), 
                             key=lambda x: -hierarchy[group]['subgroups'][x]['count']):
            sorted_subgroups[subgroup] = hierarchy[group]['subgroups'][subgroup]
        
        sorted_hierarchy[group] = {
            'count': hierarchy[group]['count'],
            'phrases': hierarchy[group]['phrases'],
            'subgroups': sorted_subgroups
        }
    
    return sorted_hierarchy

def display_hierarchy(hierarchy, excluded_phrases=None):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é —Å —á–µ–∫–±–æ–∫—Å–∞–º–∏ –±–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö expanders"""
    if excluded_phrases is None:
        excluded_phrases = set()
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    container = st.container()
    
    for group, data in hierarchy.items():
        with container:
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≥—Ä—É–ø–ø—É
            col1, col2 = st.columns([1, 4])
            with col1:
                group_excluded = st.checkbox(
                    f"{group} ({data['count']})", 
                    value=all(phrase in excluded_phrases for phrase in data['phrases']),
                    key=f"group_{group}"
                )
            
            if group_excluded:
                excluded_phrases.update(data['phrases'])
            else:
                excluded_phrases.difference_update(data['phrases'])
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø–æ–¥–≥—Ä—É–ø–ø—ã
            if data['subgroups']:
                subgroup_container = st.container()
                with subgroup_container:
                    st.write("–ü–æ–¥–≥—Ä—É–ø–ø—ã:")
                    for subgroup, sub_data in data['subgroups'].items():
                        sub_col1, sub_col2 = st.columns([1, 4])
                        with sub_col1:
                            sub_excluded = st.checkbox(
                                f"{subgroup} ({sub_data['count']})",
                                value=all(phrase in excluded_phrases for phrase in sub_data['phrases']),
                                key=f"subgroup_{group}_{subgroup}"
                            )
                        
                        if sub_excluded:
                            excluded_phrases.update(sub_data['phrases'])
                        else:
                            excluded_phrases.difference_update(sub_data['phrases'])
                        
                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ñ—Ä–∞–∑—ã –ø–æ–¥–≥—Ä—É–ø–ø—ã
                        if sub_data['phrases']:
                            with st.expander(f"–ü–æ–∫–∞–∑–∞—Ç—å —Ñ—Ä–∞–∑—ã ({len(sub_data['phrases'])})"):
                                for phrase in sub_data['phrases']:
                                    st.write(phrase)
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ñ—Ä–∞–∑—ã –≥—Ä—É–ø–ø—ã
            if data['phrases']:
                with st.expander(f"–§—Ä–∞–∑—ã –≥—Ä—É–ø–ø—ã ({len(data['phrases'])})"):
                    for phrase in data['phrases']:
                        st.write(phrase)
    
    return excluded_phrases

def semantic_core_grouper():
    st.header("üìä –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞")
    
    example_phrases = """
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio 120 80 215
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio ks 6690m
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã –≤–∏–ª–∞–¥–∂–∏–æ
—Å–±–æ—Ä–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã –≤–∏–ª–ª–∞–¥–∂–∏–æ
—Ä–µ–º–æ–Ω—Ç –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã
—Ä–µ–º–æ–Ω—Ç –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã villagio
—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã
—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—É—à–µ–≤–æ–π –∫–∞–±–∏–Ω—ã –≤–∏–ª–∞–¥–∂–∏–æ
"""
    
    st.subheader("1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ")
    input_type = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö", ["–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö", "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª"], key="data_source")
    
    if input_type == "–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö":
        phrases = parse_semantic_core(example_phrases)
    else:
        uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª", type=['txt'], key="core_uploader")
        if uploaded_file:
            text = uploaded_file.read().decode('utf-8')
            phrases = parse_semantic_core(text)
        else:
            st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª")
            return
    
    if not phrases:
        st.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–∑ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
        return
    
    st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(phrases)} —Ñ—Ä–∞–∑")
    
    st.subheader("2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ñ—Ä–∞–∑")
    hierarchy = build_hierarchy(phrases)
    
   if 'excluded_phrases' not in st.session_state:
        st.session_state.excluded_phrases = set()
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
    hierarchy_container = st.container()
    with hierarchy_container:
        st.session_state.excluded_phrases = display_hierarchy(
            hierarchy,
            st.session_state.excluded_phrases
        )
    
    st.subheader("3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è", key="clear_exclusions"):
            st.session_state.excluded_phrases = set()
            st.experimental_rerun()
    
    with col2:
        if st.button("–ò—Å–∫–ª—é—á–∏—Ç—å –≤—Å–µ —Ñ—Ä–∞–∑—ã", key="exclude_all"):
            st.session_state.excluded_phrases = set(phrases)
            st.experimental_rerun()
    
    st.subheader("4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    
    tab1, tab2 = st.tabs(["–ò—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã", "–û—Å—Ç–∞–≤—à–∏–µ—Å—è —Ñ—Ä–∞–∑—ã"])
    
    with tab1:
        if st.session_state.excluded_phrases:
            st.write("\n".join(sorted(st.session_state.excluded_phrases)))
        else:
            st.info("–ù–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑")
    
    with tab2:
        remaining = set(phrases) - st.session_state.excluded_phrases
        if remaining:
            st.write("\n".join(sorted(remaining)))
        else:
            st.warning("–í—Å–µ —Ñ—Ä–∞–∑—ã –∏—Å–∫–ª—é—á–µ–Ω—ã")

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 3: –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ =====
def hash_phone(phone):
    phone_str = str(phone).strip()
    if not phone_str:
        return ""
    digits = re.sub(r'\D', '', phone_str)
    if not digits:
        return ""
    return hashlib.sha256(digits.encode('utf-8')).hexdigest()

def get_table_download_link(df):
    csv = df.to_csv(index=False, encoding='utf-8-sig')
    b64 = base64.b64encode(csv.encode('utf-8-sig')).decode()
    return f'<a href="data:file/csv;base64,{b64}" download="processed_data.csv">–°–∫–∞—á–∞—Ç—å CSV</a>'

# ===== –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å =====
st.title("üîß –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã")
st.markdown("---")

tool_options = [
    "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤",
    "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞", 
    "–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤"
]

tool = st.sidebar.radio(
    "–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
    tool_options,
    index=0
)

if tool == "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤":
    st.header("üìâ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ –¥–ª—è —Ä–µ–∫–ª–∞–º—ã")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("1. –í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        phrases = st.text_area(
            "–ö–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏",
            height=200,
            help="–í—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
            key="phrases_input"
        )
        
    with col2:
        st.subheader("2. –£–∫–∞–∂–∏—Ç–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
        exclude = st.text_area(
            "–§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2, (–≤–∞—Ä–∏–∞–Ω—Ç1|–≤–∞—Ä–∏–∞–Ω—Ç2), /—Ä–µ–≥—ç–∫—Å–ø/",
            height=200,
            help="""–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
            - –ó–∞–ø—è—Ç—ã–µ: —Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2
            - –í–∞—Ä–∏–∞–Ω—Ç—ã: (–≤–∏–ª–ª–∞–¥–∂–∏–æ|villagio)
            - –ú–∞—Å–∫–∏: —Ä–µ–º–æ–Ω—Ç*
            - –†–µ–≥—É–ª—è—Ä–∫–∏: /—Ü–µ–Ω–∞|—Å—Ç–æ–∏–º–æ—Å—Ç—å/
            - –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: !–æ–Ω–ª–∞–π–Ω""",
            key="exclude_input"
        )
    
    st.markdown("---")
    st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    col_set1, col_set2 = st.columns(2)
    with col_set1:
        min_length = st.slider(
            "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞",
            min_value=1,
            max_value=10,
            value=3,
            help="–°–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –±—É–¥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è",
            key="min_length_slider"
        )
    
    with col_set2:
        show_stats = st.checkbox(
            "–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º",
            value=True,
            help="–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤",
            key="show_stats_checkbox"
        )
    
    if st.button("üöÄ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞", type="primary", key="generate_minus_words"):
        if not phrases:
            st.error("–í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
        else:
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
                try:
                    morph = prepare_morph_analyzer()
                    exclude_list = parse_exclude_input(exclude)
                    exclude_patterns = get_exclusion_patterns(exclude_list, morph)
                    phrases_list = [p.strip() for p in phrases.split('\n') if p.strip()]
                    
                    minus_words = process_phrases(phrases_list, exclude_patterns, morph, min_length)
                    result = " ".join([f"-{word}" for word in minus_words])
                    
                    st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
                    
                    tab1, tab2 = st.tabs(["–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç", "–°–ø–∏—Å–æ–∫ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è"])
                    
                    with tab1:
                        st.text_area(
                            "–†–µ–∑—É–ª—å—Ç–∞—Ç (—Ç–µ–∫—Å—Ç)",
                            value=result,
                            height=100,
                            key="result_text"
                        )
                    
                    with tab2:
                        st.text_area(
                            "–†–µ–∑—É–ª—å—Ç–∞—Ç (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)",
                            value="\n".join([f"-{word}" for word in minus_words]),
                            height=200,
                            key="result_list"
                        )
                    
                    if show_stats:
                        st.markdown("---")
                        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º")
                        
                        words_count = Counter()
                        for phrase in phrases_list:
                            words = re.findall(r'\b\w+\b', phrase.lower())
                            for word in words:
                                if len(word) >= min_length:
                                    normalized = normalize_word(word, morph)
                                    if not should_exclude(normalized, exclude_patterns):
                                        words_count[normalized] += 1
                        
                        df_stats = pd.DataFrame.from_dict(words_count, orient='index', columns=['–ß–∞—Å—Ç–æ—Ç–∞'])
                        df_stats = df_stats.sort_values(by='–ß–∞—Å—Ç–æ—Ç–∞', ascending=False)
                        
                        st.dataframe(df_stats.head(50))
                        st.bar_chart(df_stats.head(20))
                
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

elif tool == "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞":
    semantic_core_grouper()

elif tool == "–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤":
    st.header("üìû –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤")
    
    uploaded_file = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (Excel –∏–ª–∏ CSV)",
        type=["xlsx", "xls", "csv"],
        accept_multiple_files=False,
        key="phone_hash_uploader"
    )
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                content = uploaded_file.getvalue()
                result = chardet.detect(content)
                encoding = result['encoding'] if result['confidence'] > 0.7 else 'utf-8'
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding=encoding)
            else:
                df = pd.read_excel(uploaded_file)
            
            st.success(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
                st.dataframe(df.head())
            
            with col2:
                st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                phone_column = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏",
                    df.columns,
                    index=0,
                    key="phone_column_select"
                )
                
                new_column_name = st.text_input(
                    "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ö—ç—à–∞–º–∏",
                    value="phone_hash",
                    key="new_column_name_input"
                )
                
                if st.button("üîí –•—ç—à–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ", type="primary", key="hash_phones_button"):
                    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                        try:
                            result_df = df.copy()
                            result_df[new_column_name] = result_df[phone_column].apply(hash_phone)
                            st.success("–ì–æ—Ç–æ–≤–æ! –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
                            st.dataframe(result_df.head())
                            st.markdown(get_table_download_link(result_df), unsafe_allow_html=True)
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown("""
<style>
.footer {
    font-size: small;
    color: gray;
    text-align: center;
}
</style>
<div class="footer">
    –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã | 2023
</div>
""", unsafe_allow_html=True)
