import streamlit as st
import re
from collections import Counter, defaultdict
import pymorphy3
import pandas as pd
import hashlib
import base64
from io import StringIO
import chardet

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã",
    page_icon="üîß",
    layout="wide"
)

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 1: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ =====
def prepare_morph_analyzer():
    return pymorphy3.MorphAnalyzer()

def normalize_word(word, morph, force_exact=False):
    if force_exact or word.startswith('!'):
        return word.lstrip('!').lower()
    parsed = morph.parse(word.lower())[0]
    return parsed.normal_form

def parse_exclude_input(user_input):
    if not user_input:
        return []
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞
    if '\n' in user_input:
        lines = [line.strip() for line in user_input.split('\n') if line.strip()]
        return [item for line in lines for item in parse_exclude_input(line)]
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤ —Å–∫–æ–±–∫–∞—Ö (word1|word2)
    if '(' in user_input and ')' in user_input and '|' in user_input:
        parts = []
        remaining = user_input
        while '(' in remaining and ')' in remaining:
            before = remaining[:remaining.index('(')]
            variant_part = remaining[remaining.index('(')+1:remaining.index(')')]
            after = remaining[remaining.index(')')+1:]
            
            if before.strip():
                parts.extend(parse_exclude_input(before))
            parts.extend(v.strip() for v in variant_part.split('|') if v.strip())
            remaining = after
        if remaining.strip():
            parts.extend(parse_exclude_input(remaining))
        return parts
    
    # –û–±—ã—á–Ω—ã–π –≤–≤–æ–¥ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –∏–ª–∏ |
    delimiters = [',', '|'] if '|' in user_input else [',']
    for delim in delimiters:
        if delim in user_input:
            return [item.strip() for item in user_input.split(delim) if item.strip()]
    
    return [user_input.strip()] if user_input.strip() else []

def get_exclusion_patterns(exclude_list, morph):
    patterns = []
    for item in exclude_list:
        item = item.strip()
        if not item:
            continue
            
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
        if item.startswith('/') and item.endswith('/'):
            pattern = item[1:-1]
            patterns.append((pattern, True, True))
            continue
            
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        force_exact = item.startswith('!')
        if force_exact:
            item = item[1:]
            
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å–æ–∫ *
        if '*' in item:
            escaped = re.escape(item)
            pattern = escaped.replace(r'\*', r'\w*')
            pattern = r'\b' + pattern + r'\b'
            patterns.append((pattern, False, True))
        else:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã
            words = re.findall(r'\w+', item.lower())
            normalized_words = [normalize_word(w, morph, force_exact) for w in words]
            pattern = r'\b' + r'\s+'.join(normalized_words) + r'\b'
            patterns.append((pattern, False, False))
    
    return patterns

def should_exclude(word, exclude_patterns):
    word_lower = word.lower()
    for pattern, is_regex, is_wildcard in exclude_patterns:
        try:
            if is_regex:
                if re.search(pattern, word_lower, re.IGNORECASE):
                    return True
            elif is_wildcard:
                if re.search(pattern, word_lower, re.IGNORECASE):
                    return True
            else:
                if re.fullmatch(pattern, word_lower, re.IGNORECASE):
                    return True
        except re.error:
            continue
    return False

def process_phrases(phrases, exclude_patterns, morph, min_word_length=3):
    words_counter = Counter()
    for phrase in phrases:
        words = re.findall(r'\b\w+\b', phrase.lower())
        for word in words:
            if len(word) >= min_word_length and not should_exclude(word, exclude_patterns):
                normalized = normalize_word(word, morph)
                words_counter[normalized] += 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ, –∑–∞—Ç–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    return [word for word, count in sorted(words_counter.items(), key=lambda x: (-x[1], x[0]))]

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 2: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ =====
def parse_semantic_core(text):
    """Parse semantic core text into a list of phrases"""
    phrases = [line.strip() for line in text.split('\n') if line.strip()]
    return phrases

def build_hierarchy(phrases):
    """Build a hierarchical structure from phrases"""
    hierarchy = defaultdict(lambda: defaultdict(list))
    
    for phrase in phrases:
        parts = phrase.split()
        if len(parts) > 0:
            first_word = parts[0]
            rest = ' '.join(parts[1:]) if len(parts) > 1 else ''
            
            if rest:
                hierarchy[first_word][rest].append(phrase)
            else:
                hierarchy[first_word]['__LEAVES__'].append(phrase)
    
    return hierarchy

def display_hierarchy(hierarchy, level=0, excluded_words=None):
    """Recursively display hierarchy with checkboxes"""
    if excluded_words is None:
        excluded_words = set()
    
    for group, subgroups in sorted(hierarchy.items()):
        # Check if any phrase in this group is already excluded
        group_excluded = any(phrase in excluded_words for phrase in subgroups.get('__LEAVES__', []))
        
        # Create a unique key for the checkbox
        checkbox_key = f"group_{level}_{group}"
        
        # Display group checkbox
        excluded = st.checkbox(group, value=group_excluded, key=checkbox_key)
        
        if excluded:
            # Add all phrases in this group to excluded words
            if '__LEAVES__' in subgroups:
                excluded_words.update(subgroups['__LEAVES__'])
            
            # Add all nested phrases to excluded words
            for subgroup, phrases in subgroups.items():
                if subgroup != '__LEAVES__':
                    excluded_words.update(phrases)
        else:
            # Remove from excluded words if unchecked
            if '__LEAVES__' in subgroups:
                excluded_words.difference_update(subgroups['__LEAVES__'])
            
            for subgroup, phrases in subgroups.items():
                if subgroup != '__LEAVES__':
                    excluded_words.difference_update(phrases)
            
            # Display subgroups if any
            if len(subgroups) > 1 or (len(subgroups) == 1 and '__LEAVES__' not in subgroups):
                with st.expander(f"–ü–æ–¥–≥—Ä—É–ø–ø—ã –¥–ª—è '{group}'"):
                    for subgroup, phrases in subgroups.items():
                        if subgroup == '__LEAVES__':
                            continue
                            
                        # Check if any phrase in this subgroup is excluded
                        subgroup_excluded = any(phrase in excluded_words for phrase in phrases)
                        
                        # Create unique key for subgroup checkbox
                        subgroup_key = f"subgroup_{level}_{group}_{subgroup}"
                        
                        # Display subgroup checkbox
                        sub_excluded = st.checkbox(
                            subgroup, 
                            value=subgroup_excluded,
                            key=subgroup_key
                        )
                        
                        if sub_excluded:
                            excluded_words.update(phrases)
                        else:
                            excluded_words.difference_update(phrases)
                        
                        # Display phrases in this subgroup
                        with st.expander(f"–§—Ä–∞–∑—ã –≤ '{subgroup}'"):
                            for phrase in phrases:
                                st.write(phrase)
    
    return excluded_words

def semantic_core_grouper():
    st.header("üìä –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞")
    st.write("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ (–∫–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ)")
    
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —è–¥—Ä–æ–º", type=['txt'], key="semantic_core_uploader")
    if uploaded_file is not None:
        text = uploaded_file.read().decode('utf-8')
        phrases = parse_semantic_core(text)
        
        if phrases:
            st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(phrases)} —Ñ—Ä–∞–∑")
            
            # Initialize session state for excluded words
            if 'excluded_words' not in st.session_state:
                st.session_state.excluded_words = set()
            
            # Build hierarchy
            hierarchy = build_hierarchy(phrases)
            
            st.subheader("–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ñ—Ä–∞–∑")
            excluded_words = display_hierarchy(
                hierarchy, 
                excluded_words=st.session_state.excluded_words
            )
            
            # Update session state
            st.session_state.excluded_words = excluded_words
            
            # Display excluded words
            st.subheader("–ò—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã")
            if st.session_state.excluded_words:
                st.write("\n".join(sorted(st.session_state.excluded_words)))
                if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ", key="clear_excluded"):
                    st.session_state.excluded_words = set()
                    st.experimental_rerun()
            else:
                st.write("–ù–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑")
            
            # Display remaining phrases
            st.subheader("–û—Å—Ç–∞–≤—à–∏–µ—Å—è —Ñ—Ä–∞–∑—ã")
            all_phrases = set(phrases)
            remaining_phrases = all_phrases - st.session_state.excluded_words
            st.write("\n".join(sorted(remaining_phrases)))
        else:
            st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–∑ –≤ —Ñ–∞–π–ª–µ")

# ===== –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 3: –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ =====
def hash_phone(phone):
    phone_str = str(phone).strip()
    if not phone_str:
        return ""
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–º–µ—Ä–∞ (—É–¥–∞–ª—è–µ–º –≤—Å—ë, –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä)
    digits = re.sub(r'\D', '', phone_str)
    if not digits:
        return ""
    return hashlib.sha256(digits.encode('utf-8')).hexdigest()

def get_table_download_link(df):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è DataFrame"""
    csv = df.to_csv(index=False, encoding='utf-8-sig')
    b64 = base64.b64encode(csv.encode('utf-8-sig')).decode()
    return f'<a href="data:file/csv;base64,{b64}" download="processed_data.csv">–°–∫–∞—á–∞—Ç—å CSV</a>'

# ===== –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å =====
st.title("üîß –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã")
st.markdown("---")

# –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤ –º–µ–Ω—é
tool_options = [
    "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤",
    "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞",
    "–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤"
]

tool = st.sidebar.radio(
    "–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç",
    tool_options,
    index=0
)

if tool == "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤":
    st.header("üìâ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–∏–Ω—É—Å-—Å–ª–æ–≤ –¥–ª—è —Ä–µ–∫–ª–∞–º—ã")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("1. –í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        phrases = st.text_area(
            "–ö–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏",
            height=200,
            help="–í—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
            key="phrases_input"
        )
        
    with col2:
        st.subheader("2. –£–∫–∞–∂–∏—Ç–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
        exclude = st.text_area(
            "–§–æ—Ä–º–∞—Ç: —Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2, (–≤–∞—Ä–∏–∞–Ω—Ç1|–≤–∞—Ä–∏–∞–Ω—Ç2), /—Ä–µ–≥—ç–∫—Å–ø/",
            height=200,
            help="""–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
            - –ó–∞–ø—è—Ç—ã–µ: —Å–ª–æ–≤–æ1, —Å–ª–æ–≤–æ2
            - –í–∞—Ä–∏–∞–Ω—Ç—ã: (–≤–∏–ª–ª–∞–¥–∂–∏–æ|villagio)
            - –ú–∞—Å–∫–∏: —Ä–µ–º–æ–Ω—Ç*
            - –†–µ–≥—É–ª—è—Ä–∫–∏: /—Ü–µ–Ω–∞|—Å—Ç–æ–∏–º–æ—Å—Ç—å/
            - –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: !–æ–Ω–ª–∞–π–Ω""",
            key="exclude_input"
        )
    
    st.markdown("---")
    st.subheader("3. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    col_set1, col_set2 = st.columns(2)
    with col_set1:
        min_length = st.slider(
            "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞",
            min_value=1,
            max_value=10,
            value=3,
            help="–°–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –±—É–¥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è",
            key="min_length_slider"
        )
    
    with col_set2:
        show_stats = st.checkbox(
            "–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º",
            value=True,
            help="–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤",
            key="show_stats_checkbox"
        )
    
    if st.button("üöÄ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω—É—Å-—Å–ª–æ–≤–∞", type="primary", key="generate_minus_words"):
        if not phrases:
            st.error("–í–≤–µ–¥–∏—Ç–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
        else:
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
                try:
                    morph = prepare_morph_analyzer()
                    exclude_list = parse_exclude_input(exclude)
                    exclude_patterns = get_exclusion_patterns(exclude_list, morph)
                    phrases_list = [p.strip() for p in phrases.split('\n') if p.strip()]
                    
                    minus_words = process_phrases(phrases_list, exclude_patterns, morph, min_length)
                    result = " ".join([f"-{word}" for word in minus_words])
                    
                    st.success("‚úÖ –ì–æ—Ç–æ–≤–æ!")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –¥–≤—É—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö
                    tab1, tab2 = st.tabs(["–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç", "–°–ø–∏—Å–æ–∫ –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è"])
                    
                    with tab1:
                        st.text_area(
                            "–†–µ–∑—É–ª—å—Ç–∞—Ç (—Ç–µ–∫—Å—Ç)",
                            value=result,
                            height=100,
                            key="result_text"
                        )
                    
                    with tab2:
                        st.text_area(
                            "–†–µ–∑—É–ª—å—Ç–∞—Ç (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)",
                            value="\n".join([f"-{word}" for word in minus_words]),
                            height=200,
                            key="result_list"
                        )
                    
                    if show_stats:
                        st.markdown("---")
                        st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º")
                        
                        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                        words_count = Counter()
                        for phrase in phrases_list:
                            words = re.findall(r'\b\w+\b', phrase.lower())
                            for word in words:
                                if len(word) >= min_length:
                                    normalized = normalize_word(word, morph)
                                    if not should_exclude(normalized, exclude_patterns):
                                        words_count[normalized] += 1
                        
                        df_stats = pd.DataFrame.from_dict(words_count, orient='index', columns=['–ß–∞—Å—Ç–æ—Ç–∞'])
                        df_stats = df_stats.sort_values(by='–ß–∞—Å—Ç–æ—Ç–∞', ascending=False)
                        
                        st.dataframe(df_stats.head(50))
                        
                        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ç–æ–ø-20 —Å–ª–æ–≤
                        st.bar_chart(df_stats.head(20))
                
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

elif tool == "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞":
    semantic_core_grouper()

elif tool == "–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤":
    st.header("üìû –•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤")
    
    uploaded_file = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª (Excel –∏–ª–∏ CSV)",
        type=["xlsx", "xls", "csv"],
        accept_multiple_files=False,
        key="phone_hash_uploader"
    )
    
    if uploaded_file:
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            if uploaded_file.name.endswith('.csv'):
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è CSV
                content = uploaded_file.getvalue()
                result = chardet.detect(content)
                encoding = result['encoding'] if result['confidence'] > 0.7 else 'utf-8'
                
                # –ß–∏—Ç–∞–µ–º CSV —Å —É—á–µ—Ç–æ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding=encoding)
            else:
                df = pd.read_excel(uploaded_file)
            
            st.success(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
                st.dataframe(df.head())
            
            with col2:
                st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                phone_column = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º–∏",
                    df.columns,
                    index=0,
                    key="phone_column_select"
                )
                
                new_column_name = st.text_input(
                    "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ö—ç—à–∞–º–∏",
                    value="phone_hash",
                    key="new_column_name_input"
                )
                
                if st.button("üîí –•—ç—à–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ", type="primary", key="hash_phones_button"):
                    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞..."):
                        try:
                            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                            result_df = df.copy()
                            
                            # –•—ç—à–∏—Ä—É–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã
                            result_df[new_column_name] = result_df[phone_column].apply(hash_phone)
                            
                            st.success("–ì–æ—Ç–æ–≤–æ! –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
                            st.dataframe(result_df.head())
                            
                            # –ö–Ω–æ–ø–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                            st.markdown(get_table_download_link(result_df), unsafe_allow_html=True)
                        
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
        
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown("""
<style>
.footer {
    font-size: small;
    color: gray;
    text-align: center;
}
</style>
<div class="footer">
    –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã | 2023
</div>
""", unsafe_allow_html=True)
